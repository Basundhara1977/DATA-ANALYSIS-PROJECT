{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9671e4b",
   "metadata": {},
   "source": [
    "# Big Data Analysis at Scale — Dask / PySpark\n",
    "\n",
    "> Internship Deliverable: Notebook with insights derived from big data processing.\n",
    "\n",
    "**What this does**\n",
    "- Auto-detects **Dask** or **PySpark** and uses whichever is available.\n",
    "- Reads a big dataset (Parquet or CSV, local or cloud), performs ETL + analytics at scale.\n",
    "- Shows partitioning, lazy evaluation, caching, and write-out of results.\n",
    "- Produces **insights** and persists outputs to Parquet/CSV.\n",
    "\n",
    "**Pro tip**: If you're running locally with limited RAM, prefer Parquet + column pruning, and start small (sample) before scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b31bf",
   "metadata": {},
   "source": [
    "## 0) Config — set your inputs/outputs\n",
    "Update the paths below. For real big-data vibes, point to a directory of multiple Parquet/CSV files.\n",
    "\n",
    "**Examples:**\n",
    "- Local: `data/nyc_taxi_parquet/2019/*/*.parquet`\n",
    "- Cloud (Spark): `s3a://ursa-labs-taxi-data/2019/01/*.parquet`\n",
    "- Cloud (Dask): `s3://ursa-labs-taxi-data/2019/01/*.parquet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# paths (update if needed)\n",
    "INPUT_PATH = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\data\\nyc_taxi\\yellow_tripdata_2019-01.parquet\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\outputs\"\n",
    "CURATED_PATH = os.path.join(OUTPUT_DIR, \"curated_parquet\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"Inputs set ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7ca62",
   "metadata": {},
   "source": [
    "## 1) Environment detection + cluster spin-up\n",
    "We try Dask first (fast to boot locally), then fall back to PySpark if available. Both paths implement similar logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a285fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"BigDataAnalysis\").getOrCreate()\n",
    "print(\"SparkSession ready ✨\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13170fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BACKEND == \"dask\":\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)\n",
    "    display(client)\n",
    "    print(f\"Dask cluster up with {len(client.nthreads())} workers ✨\")\n",
    "elif BACKEND == \"spark\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = (SparkSession.builder\n",
    "             .appName(\"BigDataAnalysisDemo\")\n",
    "             .config(\"spark.sql.sources.useV1SourceList\", \"csv,json\")\n",
    "             .getOrCreate())\n",
    "    spark\n",
    "    print(\"SparkSession ready ✨\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07a407",
   "metadata": {},
   "source": [
    "## 2) Load — column pruning + schema hints\n",
    "We prune columns to reduce IO and speed things up. Adjust `COLUMNS` to match your dataset.\n",
    "Below assumes NYC taxi-like columns as a demo; map these to your schema if different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29265cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "# check schema + preview\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(\"Raw row count:\", df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2973a74",
   "metadata": {},
   "source": [
    "## 3) ETL — cleanups, time features, and filters\n",
    "We’ll:\n",
    "- Cast dtypes\n",
    "- Compute trip duration\n",
    "- Filter out obvious outliers\n",
    "- Add hour/day/weekday features for time-series group-bys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, unix_timestamp, hour, dayofmonth, dayofweek\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "     )\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"duration_min\",\n",
    "    (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 60.0\n",
    ")\n",
    "\n",
    "# filters (relaxed to avoid empty df issues)\n",
    "df = df.filter(\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"trip_distance\") < 100) &\n",
    "    (col(\"duration_min\") > 0) &\n",
    "    (col(\"duration_min\") < 500)\n",
    ")\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"day\", dayofmonth(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"dow\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "     )\n",
    "\n",
    "df.show(5)\n",
    "print(\"Filtered row count:\", df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a65aef",
   "metadata": {},
   "source": [
    "## 4) Core analytics — insights\n",
    "We compute:\n",
    "- Median / 95th pct of trip distance & duration\n",
    "- Hourly ride volume\n",
    "- Top pickup zones\n",
    "These are classic, scalable group-bys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentiles\n",
    "q_td = df.approxQuantile(\"trip_distance\",[0.5,0.95],0.01)\n",
    "q_du = df.approxQuantile(\"duration_min\",[0.5,0.95],0.01)\n",
    "\n",
    "hourly = df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "top_pu = df.groupBy(\"PULocationID\").count().orderBy(F.desc(\"count\")).limit(10)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "q_pdf = pd.DataFrame([\n",
    "    {\"column\":\"trip_distance\",\"p50\":q_td[0],\"p95\":q_td[1]},\n",
    "    {\"column\":\"duration_min\",\"p50\":q_du[0],\"p95\":q_du[1]}\n",
    "])\n",
    "hourly_pdf = hourly.toPandas()\n",
    "top_pu_pdf = top_pu.toPandas()\n",
    "\n",
    "print(\"Percentiles:\\n\", q_pdf)\n",
    "print(\"\\nHourly volume:\\n\", hourly_pdf)\n",
    "print(\"\\nTop pickup zones:\\n\", top_pu_pdf)\n",
    "\n",
    "# save\n",
    "q_pdf.to_csv(os.path.join(OUTPUT_DIR, \"percentiles.csv\"), index=False)\n",
    "hourly_pdf.to_csv(os.path.join(OUTPUT_DIR, \"hourly_volume.csv\"), index=False)\n",
    "top_pu_pdf.to_csv(os.path.join(OUTPUT_DIR, \"top_pickups.csv\"), index=False)\n",
    "print(\"✅ Saved CSVs to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a419f3",
   "metadata": {},
   "source": [
    "## 5) Scalability flex — partitions & caching\n",
    "Show how performance changes with different partition counts and caching/persisting. This is \n",
    "a **demonstration block** — tweak `N_PARTITIONS` based on your machine or cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.repartition(8).persist()\n",
    "import time\n",
    "t0 = time.time()\n",
    "_ = df2.agg(F.avg(\"fare_amount\").alias(\"avg_fare\")).collect()\n",
    "print(f\"Spark avg(fare_amount) with 8 partitions took {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da2734",
   "metadata": {},
   "source": [
    "## 6) Save curated dataset\n",
    "Good practice: write a cleaned, analytics-ready table for downstream stuff (dashboards/ML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8dc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(CURATED_PATH)\n",
    "print(\"✅ Wrote curated dataset to:\", CURATED_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037aabe0",
   "metadata": {},
   "source": [
    "## 7) Insights (interpretation)\n",
    "When you run the notebook on a real dataset, fill in your observations below. Example:\n",
    "- **P50 trip distance**: X km; **P95**: Y km → long-tail trips exist but are rare.\n",
    "- **Peak demand hour**: HH:00 local → staffing & surge pricing opportunity.\n",
    "- **Top pickup zones**: IDs [A, B, C] → consider geo-targeted supply.\n",
    "\n",
    "This section is meant for you to contextualize numbers for business stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd7b6c",
   "metadata": {},
   "source": [
    "---\n",
    "### Appendix: Switching between Dask and PySpark\n",
    "- Install Dask: `pip install dask[complete] distributed pyarrow fsspec s3fs`\n",
    "- Install Spark: `pip install pyspark pyarrow`\n",
    "- Spark + S3: configure `hadoop-aws` + AWS creds or use local data.\n",
    "- Parquet >>> CSV for performance; leverage partitioned folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f67eb-0739-45b5-8bc5-49b89acfa0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fd170-92a9-47ca-abad-52e587b15e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Row count:\", df.count())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc52a69-3d70-4bc2-ade0-51bd7fd4e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Row count:\", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989b037-a070-4ab1-b10a-2e60d67e2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ======================\n",
    "# CONFIG\n",
    "# ======================\n",
    "INPUT_PATH = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\data\\nyc_taxi\\yellow_tripdata_2019-01.parquet\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\outputs\"\n",
    "CURATED_PATH = os.path.join(OUTPUT_DIR, \"curated_parquet\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ======================\n",
    "# START SPARK\n",
    "# ======================\n",
    "spark = SparkSession.builder.appName(\"BigDataAnalysis\").getOrCreate()\n",
    "print(\"SparkSession ready ✨\", spark.version)\n",
    "\n",
    "# ======================\n",
    "# LOAD DATA\n",
    "# ======================\n",
    "df = spark.read.parquet(INPUT_PATH)\n",
    "print(\"Raw row count:\", df.count())\n",
    "\n",
    "# ======================\n",
    "# ETL (cleaning + features)\n",
    "# ======================\n",
    "from pyspark.sql.functions import to_timestamp, col, unix_timestamp, hour, dayofmonth, dayofweek\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "     )\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"duration_min\",\n",
    "    (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 60.0\n",
    ")\n",
    "\n",
    "# filters (kept broad so df isn’t empty)\n",
    "df = df.filter(\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"trip_distance\") < 100) &\n",
    "    (col(\"duration_min\") > 0) &\n",
    "    (col(\"duration_min\") < 500)\n",
    ")\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"day\", dayofmonth(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"dow\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "     )\n",
    "\n",
    "print(\"Filtered row count:\", df.count())\n",
    "df.show(5)\n",
    "\n",
    "# ======================\n",
    "# CORE ANALYTICS\n",
    "# ======================\n",
    "q_td = df.approxQuantile(\"trip_distance\",[0.5,0.95],0.01)\n",
    "q_du = df.approxQuantile(\"duration_min\",[0.5,0.95],0.01)\n",
    "\n",
    "hourly = df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "top_pu = df.groupBy(\"PULocationID\").count().orderBy(F.desc(\"count\")).limit(10)\n",
    "\n",
    "import pandas as pd\n",
    "q_pdf = pd.DataFrame([\n",
    "    {\"column\":\"trip_distance\",\"p50\":q_td[0],\"p95\":q_td[1]},\n",
    "    {\"column\":\"duration_min\",\"p50\":q_du[0],\"p95\":q_du[1]}\n",
    "])\n",
    "hourly_pdf = hourly.toPandas()\n",
    "top_pu_pdf = top_pu.toPandas()\n",
    "\n",
    "print(\"\\nPercentiles:\\n\", q_pdf)\n",
    "print(\"\\nHourly volume:\\n\", hourly_pdf)\n",
    "print(\"\\nTop pickup zones:\\n\", top_pu_pdf)\n",
    "\n",
    "# save outputs\n",
    "q_pdf.to_csv(os.path.join(OUTPUT_DIR, \"percentiles.csv\"), index=False)\n",
    "hourly_pdf.to_csv(os.path.join(OUTPUT_DIR, \"hourly_volume.csv\"), index=False)\n",
    "top_pu_pdf.to_csv(os.path.join(OUTPUT_DIR, \"top_pickups.csv\"), index=False)\n",
    "print(\"✅ Saved CSVs to:\", OUTPUT_DIR)\n",
    "\n",
    "# ======================\n",
    "# SAVE CURATED DATASET\n",
    "# ======================\n",
    "df.write.mode(\"overwrite\").parquet(CURATED_PATH)\n",
    "print(\"✅ Wrote curated dataset to:\", CURATED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353c6a7-cd92-4051-a285-f9ccbf211cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ======================\n",
    "# CONFIG\n",
    "# ======================\n",
    "INPUT_PATH = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\data\\nyc_taxi\\yellow_tripdata_2019-01.parquet\"\n",
    "\n",
    "# write outputs to a simple path (not OneDrive)\n",
    "OUTPUT_DIR = r\"C:\\bigdata_outputs\"\n",
    "CURATED_PATH = os.path.join(OUTPUT_DIR, \"curated_parquet\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ======================\n",
    "# START SPARK\n",
    "# ======================\n",
    "spark = SparkSession.builder.appName(\"BigDataAnalysis\").getOrCreate()\n",
    "print(\"SparkSession ready ✨\", spark.version)\n",
    "\n",
    "# ======================\n",
    "# LOAD DATA\n",
    "# ======================\n",
    "df = spark.read.parquet(INPUT_PATH)\n",
    "print(\"Raw row count:\", df.count())\n",
    "\n",
    "# ======================\n",
    "# ETL (cleaning + features)\n",
    "# ======================\n",
    "from pyspark.sql.functions import to_timestamp, col, unix_timestamp, hour, dayofmonth, dayofweek\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "     )\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"duration_min\",\n",
    "    (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 60.0\n",
    ")\n",
    "\n",
    "# filters (broad so df isn’t empty)\n",
    "df = df.filter(\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"trip_distance\") < 100) &\n",
    "    (col(\"duration_min\") > 0) &\n",
    "    (col(\"duration_min\") < 500)\n",
    ")\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"day\", dayofmonth(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"dow\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "     )\n",
    "\n",
    "print(\"Filtered row count:\", df.count())\n",
    "df.show(5)\n",
    "\n",
    "# ======================\n",
    "# CORE ANALYTICS\n",
    "# ======================\n",
    "q_td = df.approxQuantile(\"trip_distance\",[0.5,0.95],0.01)\n",
    "q_du = df.approxQuantile(\"duration_min\",[0.5,0.95],0.01)\n",
    "\n",
    "hourly = df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "top_pu = df.groupBy(\"PULocationID\").count().orderBy(F.desc(\"count\")).limit(10)\n",
    "\n",
    "import pandas as pd\n",
    "q_pdf = pd.DataFrame([\n",
    "    {\"column\":\"trip_distance\",\"p50\":q_td[0],\"p95\":q_td[1]},\n",
    "    {\"column\":\"duration_min\",\"p50\":q_du[0],\"p95\":q_du[1]}\n",
    "])\n",
    "hourly_pdf = hourly.toPandas()\n",
    "top_pu_pdf = top_pu.toPandas()\n",
    "\n",
    "print(\"\\nPercentiles:\\n\", q_pdf)\n",
    "print(\"\\nHourly volume:\\n\", hourly_pdf)\n",
    "print(\"\\nTop pickup zones:\\n\", top_pu_pdf)\n",
    "\n",
    "# save outputs\n",
    "q_pdf.to_csv(os.path.join(OUTPUT_DIR, \"percentiles.csv\"), index=False)\n",
    "hourly_pdf.to_csv(os.path.join(OUTPUT_DIR, \"hourly_volume.csv\"), index=False)\n",
    "top_pu_pdf.to_csv(os.path.join(OUTPUT_DIR, \"top_pickups.csv\"), index=False)\n",
    "print(\"✅ Saved CSVs to:\", OUTPUT_DIR)\n",
    "\n",
    "# ======================\n",
    "# SAVE CURATED DATASET\n",
    "# ======================\n",
    "print(\"Final row count to save:\", df.count())\n",
    "df.write.mode(\"overwrite\").parquet(CURATED_PATH)\n",
    "print(\"✅ Wrote curated dataset to:\", CURATED_PATH)\n",
    "\n",
    "# ======================\n",
    "# TEST SMALL CSV (sanity check)\n",
    "# ======================\n",
    "df.limit(10).toPandas().to_csv(os.path.join(OUTPUT_DIR, \"test_out.csv\"), index=False)\n",
    "print(\"✅ Wrote test_out.csv for quick check\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a73bed-6e58-4658-b746-8e1f4f727835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- BIG DATA ANALYSIS (ONE-CELL, SPARK) --------\n",
    "import os, sys, glob, time, traceback\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import to_timestamp, col, unix_timestamp, hour, dayofmonth, dayofweek\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "INPUT_PATH = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\data\\nyc_taxi\\yellow_tripdata_2019-01.parquet\"\n",
    "PREFERRED_OUTPUTS = [r\"C:\\bigdata_outputs\", r\"C:\\temp\\bigdata_outputs\"]  # fallbacks if blocked\n",
    "FAST_SAMPLE = False            # True = faster test run on ~1% of data\n",
    "SAMPLE_FRACTION = 0.01\n",
    "\n",
    "# ---------- PICK OUTPUT DIR THAT WORKS ----------\n",
    "OUTPUT_DIR = None\n",
    "err_msgs = []\n",
    "for cand in PREFERRED_OUTPUTS:\n",
    "    try:\n",
    "        os.makedirs(cand, exist_ok=True)\n",
    "        testfile = os.path.join(cand, \"python_write_test.txt\")\n",
    "        with open(testfile, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"ok\")\n",
    "        os.remove(testfile)\n",
    "        OUTPUT_DIR = cand\n",
    "        break\n",
    "    except Exception as e:\n",
    "        err_msgs.append(f\"{cand} -> {e}\")\n",
    "\n",
    "if OUTPUT_DIR is None:\n",
    "    print(\"❌ Could not create/write any output dir:\", err_msgs)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "CURATED_PATH = os.path.join(OUTPUT_DIR, \"curated_parquet\")\n",
    "print(\"✅ Using OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "\n",
    "# ---------- CHECK INPUT EXISTS ----------\n",
    "def expand_paths(p):\n",
    "    if any(ch in p for ch in [\"*\", \"?\", \"[\"]):\n",
    "        return glob.glob(p)\n",
    "    return [p] if os.path.exists(p) else []\n",
    "\n",
    "matches = expand_paths(INPUT_PATH)\n",
    "print(\"Input path:\", INPUT_PATH)\n",
    "print(\"Matched files:\", len(matches))\n",
    "if len(matches) == 0:\n",
    "    print(\"❌ No files found at INPUT_PATH. Listing parent dir for clues:\")\n",
    "    parent = os.path.dirname(INPUT_PATH)\n",
    "    try:\n",
    "        print(\"Parent dir:\", parent)\n",
    "        print(\"Parent dir entries (first 20):\", os.listdir(parent)[:20])\n",
    "    except Exception as e:\n",
    "        print(\"Could not list parent dir:\", e)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ---------- START SPARK ----------\n",
    "spark = SparkSession.builder.appName(\"BigDataAnalysis-OneCell\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "print(\"SparkSession ✅\", spark.version)\n",
    "\n",
    "# ---------- SPARK WRITE SMOKE TEST ----------\n",
    "try:\n",
    "    tdf = spark.createDataFrame([(1, 10.0), (2, 20.0)], [\"PULocationID\",\"fare_amount\"])\n",
    "    smoke_path = os.path.join(OUTPUT_DIR, \"spark_write_smoke\")\n",
    "    tdf.write.mode(\"overwrite\").parquet(smoke_path)\n",
    "    print(\"✅ Spark write smoke OK ->\", smoke_path)\n",
    "except Exception as e:\n",
    "    print(\"❌ Spark write smoke FAILED:\", e)\n",
    "    print(\"Traceback:\\n\", traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    df = spark.read.option(\"mergeSchema\", True).parquet(INPUT_PATH)\n",
    "    print(f\"Loaded parquet in {time.time()-t0:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to read parquet:\", e)\n",
    "    print(\"Traceback:\\n\", traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# (Optional) fast sample to speed things up locally\n",
    "if FAST_SAMPLE:\n",
    "    df = df.sample(False, SAMPLE_FRACTION, seed=42)\n",
    "    print(f\"FAST_SAMPLE on -> fraction={SAMPLE_FRACTION}\")\n",
    "\n",
    "# quick peek without scanning the world\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# ---------- ETL ----------\n",
    "df = (df\n",
    "      .withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "     )\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"duration_min\",\n",
    "    (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 60.0\n",
    ")\n",
    "\n",
    "# filters (broad to avoid empty DF on some months)\n",
    "df = df.filter(\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"trip_distance\") < 100) &\n",
    "    (col(\"duration_min\") > 0) &\n",
    "    (col(\"duration_min\") < 500)\n",
    ")\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"day\", dayofmonth(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"dow\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "     )\n",
    "\n",
    "# count can be slow; print a cheap sanity via limit first\n",
    "mini = df.limit(5).toPandas()\n",
    "print(\"Mini preview (post-ETL):\")\n",
    "print(mini)\n",
    "\n",
    "# safer row count (will take some time if large)\n",
    "rc = df.count()\n",
    "print(\"Post-ETL row count:\", rc)\n",
    "if rc == 0:\n",
    "    print(\"❌ Post-ETL DataFrame is empty. Relax filters or check timestamps in your file.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ---------- CORE ANALYTICS ----------\n",
    "q_td = df.approxQuantile(\"trip_distance\",[0.5,0.95],0.01)\n",
    "q_du = df.approxQuantile(\"duration_min\",[0.5,0.95],0.01)\n",
    "hourly = df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "top_pu = df.groupBy(\"PULocationID\").count().orderBy(F.desc(\"count\")).limit(10)\n",
    "\n",
    "q_pdf = pd.DataFrame([\n",
    "    {\"column\":\"trip_distance\",\"p50\":q_td[0],\"p95\":q_td[1]},\n",
    "    {\"column\":\"duration_min\",\"p50\":q_du[0],\"p95\":q_du[1]}\n",
    "])\n",
    "hourly_pdf = hourly.toPandas()\n",
    "top_pu_pdf = top_pu.toPandas()\n",
    "\n",
    "print(\"\\nPercentiles:\\n\", q_pdf)\n",
    "print(\"\\nHourly volume (head):\\n\", hourly_pdf.head())\n",
    "print(\"\\nTop pickup zones:\\n\", top_pu_pdf)\n",
    "\n",
    "# ---------- SAVE CSV OUTPUTS ----------\n",
    "try:\n",
    "    q_pdf.to_csv(os.path.join(OUTPUT_DIR, \"percentiles.csv\"), index=False)\n",
    "    hourly_pdf.to_csv(os.path.join(OUTPUT_DIR, \"hourly_volume.csv\"), index=False)\n",
    "    top_pu_pdf.to_csv(os.path.join(OUTPUT_DIR, \"top_pickups.csv\"), index=False)\n",
    "    print(\"✅ Saved CSVs to:\", OUTPUT_DIR)\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed saving CSVs:\", e)\n",
    "    print(\"Traceback:\\n\", traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# ---------- SAVE CURATED PARQUET ----------\n",
    "try:\n",
    "    # coalesce to fewer files so it’s obvious in Explorer\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(CURATED_PATH)\n",
    "    print(\"✅ Wrote curated dataset to:\", CURATED_PATH)\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed writing curated parquet:\", e)\n",
    "    print(\"Traceback:\\n\", traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# ---------- FINAL LISTING ----------\n",
    "try:\n",
    "    print(\"\\nFiles in OUTPUT_DIR:\")\n",
    "    print(os.listdir(OUTPUT_DIR))\n",
    "    if os.path.isdir(CURATED_PATH):\n",
    "        print(\"Curated files (first 10):\", os.listdir(CURATED_PATH)[:10])\n",
    "except Exception as e:\n",
    "    print(\"Could not list output dir:\", e)\n",
    "\n",
    "# Optional: pop open File Explorer (Windows only)\n",
    "try:\n",
    "    os.startfile(OUTPUT_DIR)\n",
    "except Exception:\n",
    "    pass\n",
    "# -------- END --------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938cd13-9781-496f-a7f5-c16ff9c305c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Main outputs:\", os.listdir(r\"C:\\bigdata_outputs\"))\n",
    "print(\"Curated parquet folder:\", os.listdir(r\"C:\\bigdata_outputs\\curated_parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da3247-c0fd-4170-af94-d7da956a5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "INPUT_PATH = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\data\\nyc_taxi\\yellow_tripdata_2019-01.parquet\"\n",
    "\n",
    "print(\"Exists? ->\", os.path.exists(INPUT_PATH))\n",
    "print(\"Is dir? ->\", os.path.isdir(INPUT_PATH))\n",
    "\n",
    "parent = os.path.dirname(INPUT_PATH)\n",
    "print(\"Parent folder listing (first 10):\", os.listdir(parent)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db775738-45a6-478c-a2f5-f11c660c226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestWrite\").getOrCreate()\n",
    "test = spark.createDataFrame([(1,\"a\"),(2,\"b\")], [\"id\",\"val\"])\n",
    "print(\"Test count:\", test.count())\n",
    "\n",
    "test_out = r\"C:\\bigdata_outputs\\test_parquet\"\n",
    "test.write.mode(\"overwrite\").parquet(test_out)\n",
    "print(\"Wrote test parquet to:\", test_out)\n",
    "\n",
    "import os\n",
    "print(\"Contents of test_out:\", os.listdir(test_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cfc2c9-a826-4e88-b721-cb7a99e3dfed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
