{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43ee7ec1-d0c5-47e5-96d6-4f24c395a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs set ✅\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# paths (update if needed)\n",
    "INPUT_PATH = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\data\\nyc_taxi\\yellow_tripdata_2019-01.parquet\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\outputs\"\n",
    "CURATED_PATH = os.path.join(OUTPUT_DIR, \"curated_parquet\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"Inputs set ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bafa9e3b-ac4d-4969-900c-ea688903f6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession ready ✨ 4.0.0\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"BigDataAnalysis\").getOrCreate()\n",
    "print(\"SparkSession ready ✨\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd183c45-d299-4ae1-bdab-41221d4c1576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: integer (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2019-01-01 00:46:40|  2019-01-01 00:53:20|            1.0|          1.5|       1.0|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                NULL|       NULL|\n",
      "|       1| 2019-01-01 00:59:47|  2019-01-01 01:18:59|            1.0|          2.6|       1.0|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                NULL|       NULL|\n",
      "|       2| 2018-12-21 13:48:30|  2018-12-21 13:52:40|            3.0|          0.0|       1.0|                 N|         236|         236|           1|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                NULL|       NULL|\n",
      "|       2| 2018-11-28 15:52:25|  2018-11-28 15:55:45|            5.0|          0.0|       1.0|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        7.55|                NULL|       NULL|\n",
      "|       2| 2018-11-28 15:56:57|  2018-11-28 15:58:33|            5.0|          0.0|       2.0|                 N|         193|         193|           2|       52.0|  0.0|    0.5|       0.0|         0.0|                  0.3|       55.55|                NULL|       NULL|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "Raw row count: 7696617\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "# check schema + preview\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(\"Raw row count:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "593a09c8-4e4f-4514-ada0-1c2fa7775ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----+---+---+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|      duration_min|hour|day|dow|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----+---+---+\n",
      "|       1| 2019-01-01 00:46:40|  2019-01-01 00:53:20|            1.0|          1.5|       1.0|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                NULL|       NULL| 6.666666666666667|   0|  1|  3|\n",
      "|       1| 2019-01-01 00:59:47|  2019-01-01 01:18:59|            1.0|          2.6|       1.0|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                NULL|       NULL|              19.2|   0|  1|  3|\n",
      "|       1| 2019-01-01 00:21:28|  2019-01-01 00:28:37|            1.0|          1.3|       1.0|                 N|         163|         229|           1|        6.5|  0.5|    0.5|      1.25|         0.0|                  0.3|        9.05|                NULL|       NULL|              7.15|   0|  1|  3|\n",
      "|       1| 2019-01-01 00:32:01|  2019-01-01 00:45:39|            1.0|          3.7|       1.0|                 N|         229|           7|           1|       13.5|  0.5|    0.5|       3.7|         0.0|                  0.3|        18.5|                NULL|       NULL|13.633333333333333|   0|  1|  3|\n",
      "|       1| 2019-01-01 00:57:32|  2019-01-01 01:09:32|            2.0|          2.1|       1.0|                 N|         141|         234|           1|       10.0|  0.5|    0.5|       1.7|         0.0|                  0.3|        13.0|                NULL|       NULL|              12.0|   0|  1|  3|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----+---+---+\n",
      "only showing top 5 rows\n",
      "Filtered row count: 7620875\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, unix_timestamp, hour, dayofmonth, dayofweek\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "     )\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"duration_min\",\n",
    "    (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 60.0\n",
    ")\n",
    "\n",
    "# filters (relaxed to avoid empty df issues)\n",
    "df = df.filter(\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"trip_distance\") < 100) &\n",
    "    (col(\"duration_min\") > 0) &\n",
    "    (col(\"duration_min\") < 500)\n",
    ")\n",
    "\n",
    "df = (df\n",
    "      .withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"day\", dayofmonth(col(\"tpep_pickup_datetime\")))\n",
    "      .withColumn(\"dow\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "     )\n",
    "\n",
    "df.show(5)\n",
    "print(\"Filtered row count:\", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b19fdd0-21bf-46ab-b480-53c610b4e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\basun\\bigdata_project\\venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\basun\\bigdata_project\\venv\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\basun\\bigdata_project\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\basun\\bigdata_project\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\basun\\bigdata_project\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\basun\\bigdata_project\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles:\n",
      "           column        p50        p95\n",
      "0  trip_distance   1.520000  11.100000\n",
      "1   duration_min  10.233333  31.316667\n",
      "\n",
      "Hourly volume:\n",
      "     hour   count\n",
      "0      0  205281\n",
      "1      1  147303\n",
      "2      2  107786\n",
      "3      3   76657\n",
      "4      4   60033\n",
      "5      5   74058\n",
      "6      6  176053\n",
      "7      7  301419\n",
      "8      8  370458\n",
      "9      9  362755\n",
      "10    10  358095\n",
      "11    11  371960\n",
      "12    12  397400\n",
      "13    13  400173\n",
      "14    14  428754\n",
      "15    15  447962\n",
      "16    16  416246\n",
      "17    17  464137\n",
      "18    18  511222\n",
      "19    19  471618\n",
      "20    20  419865\n",
      "21    21  406520\n",
      "22    22  365937\n",
      "23    23  279183\n",
      "\n",
      "Top pickup zones:\n",
      "    PULocationID   count\n",
      "0           237  331206\n",
      "1           236  321714\n",
      "2           161  310856\n",
      "3           162  275715\n",
      "4           230  261970\n",
      "5           186  259110\n",
      "6            48  239520\n",
      "7           170  238063\n",
      "8           234  236469\n",
      "9           142  234237\n",
      "✅ Saved CSVs to: C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\outputs\n"
     ]
    }
   ],
   "source": [
    "# install pandas inside notebook (only once per env)\n",
    "!pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Percentiles\n",
    "q_td = df.approxQuantile(\"trip_distance\",[0.5,0.95],0.01)\n",
    "q_du = df.approxQuantile(\"duration_min\",[0.5,0.95],0.01)\n",
    "\n",
    "hourly = df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "top_pu = df.groupBy(\"PULocationID\").count().orderBy(F.desc(\"count\")).limit(10)\n",
    "\n",
    "q_pdf = pd.DataFrame([\n",
    "    {\"column\":\"trip_distance\",\"p50\":q_td[0],\"p95\":q_td[1]},\n",
    "    {\"column\":\"duration_min\",\"p50\":q_du[0],\"p95\":q_du[1]}\n",
    "])\n",
    "hourly_pdf = hourly.toPandas()\n",
    "top_pu_pdf = top_pu.toPandas()\n",
    "\n",
    "print(\"Percentiles:\\n\", q_pdf)\n",
    "print(\"\\nHourly volume:\\n\", hourly_pdf)\n",
    "print(\"\\nTop pickup zones:\\n\", top_pu_pdf)\n",
    "\n",
    "# save\n",
    "q_pdf.to_csv(os.path.join(OUTPUT_DIR, \"percentiles.csv\"), index=False)\n",
    "hourly_pdf.to_csv(os.path.join(OUTPUT_DIR, \"hourly_volume.csv\"), index=False)\n",
    "top_pu_pdf.to_csv(os.path.join(OUTPUT_DIR, \"top_pickups.csv\"), index=False)\n",
    "print(\"✅ Saved CSVs to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38965d21-0973-4aed-9557-4f3e9a6edd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark avg(fare_amount) with 8 partitions took 0.77s\n"
     ]
    }
   ],
   "source": [
    "df2 = df.repartition(8).persist()\n",
    "import time\n",
    "t0 = time.time()\n",
    "_ = df2.agg(F.avg(\"fare_amount\").alias(\"avg_fare\")).collect()\n",
    "print(f\"Spark avg(fare_amount) with 8 partitions took {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec84702b-be8c-4f0a-9c6f-6e55496ed0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basun\\OneDrive\\Desktop\\Programming practice\\bigdata_project\\outputs\n"
     ]
    }
   ],
   "source": [
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e239155-7524-4b4e-9e44-6bd19a99707a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
